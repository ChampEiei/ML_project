{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Modern travelers often struggle to find the best flight deals due to rapidly changing prices and a variety of airline options. The goal of this project is to **predict flight ticket prices** based on key features (e.g., departure time, days until departure, flight class, number of stops, etc.). By building a **data pipeline** that includes data ingestion, transformation, model training, and deployment, we aim to:\n",
    "\n",
    "1. **Help consumers** understand how flight prices fluctuate over time.  \n",
    "2. **Enable proactive decisions** regarding ticket purchases (e.g., buying earlier for cheaper prices).  \n",
    "3. **Demonstrate** a scalable machine learning solution that can be integrated into a real-world booking platform.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Objectives\n",
    "\n",
    "1. **Data Collection and Cleansing**  \n",
    "   - Gather raw flight data from reliable sources.  \n",
    "   - Clean and format the dataset (removing outliers, handling missing values, etc.) to ensure data quality.\n",
    "\n",
    "2. **Feature Engineering**  \n",
    "   - Create or transform relevant features such as days left until departure, time of day (morning, evening, etc.), number of stops, and flight class.  \n",
    "   - Extract or encode categorical variables (e.g., source city, destination city) to make them usable for the model.\n",
    "\n",
    "3. **Model Training and Evaluation**  \n",
    "   - Implement a scikit-learn **Pipeline** that includes preprocessing steps (e.g., `ColumnTransformer`) and a machine learning model (e.g., random forest, gradient boosting, or a tuned regression model).  \n",
    "   - Perform hyperparameter tuning using **GridSearchCV** to select the best model based on performance metrics (e.g., accuracy, RMSE, or MAE).\n",
    "\n",
    "4. **Deployment and Visualization**  \n",
    "   - Serialize the best model (e.g., via pickle) and integrate it into a **Flask** application.  \n",
    "   - Provide an interactive form where users can input flight details to receive an **instant price prediction**.  \n",
    "   - Create dashboards or plots (with libraries like **Plotly**) to visualize price distributions, the effect of days left on prices, and other insights.\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- **Accurate Flight Price Predictions**  \n",
    "  A system that forecasts ticket costs given a set of user inputs, enabling informed purchase decisions.\n",
    "\n",
    "- **Actionable Insights**  \n",
    "  Graphs and statistics that reveal how specific factors (like booking timing, number of stops, or flight class) influence ticket pricing.\n",
    "\n",
    "- **Reusable Pipeline**  \n",
    "  A robust data pipeline and trained model that can be updated with new data or integrated into larger applications.\n",
    "\n",
    "- **User-Friendly Interface**  \n",
    "  A simple web app where users can interact with the model and see real-time results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Modeling\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "**Introduction**  \n",
    "The objective of this study is to analyze the flight booking dataset obtained from the “Ease My Trip” website. This online platform is widely used by passengers to book flight tickets, and the dataset reflects real-world booking data. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction).\n",
    "\n",
    "**Study Objectives**  \n",
    "- **Statistical Analysis:** Conduct various statistical hypothesis tests to extract meaningful insights from the dataset.  \n",
    "- **Predictive Modeling:** Build and compare multiple regression models to predict flight prices accurately.  \n",
    "- **Insight Generation:** Discover valuable information that can help passengers understand price fluctuations and make informed booking decisions.\n",
    "\n",
    "**Significance**  \n",
    "A comprehensive study of this dataset will enable the discovery of patterns and trends in flight pricing, providing enormous value to both passengers and industry stakeholders. The insights derived from this analysis can help optimize pricing strategies and improve the overall booking experience.\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling Approach\n",
    "\n",
    "To predict flight ticket prices, several regression models were evaluated and incorporated into a robust data pipeline. The models used include:\n",
    "\n",
    "- **Linear Regression**: `LinearRegression()`  \n",
    "  A simple model that assumes a linear relationship between input features and flight price.\n",
    "\n",
    "- **Decision Tree Regressor**: `DecisionTreeRegressor()`  \n",
    "  A non-linear model that splits the dataset into regions based on feature thresholds.\n",
    "\n",
    "- **Random Forest Regressor**: `RandomForestRegressor()`  \n",
    "  An ensemble of decision trees that improves prediction accuracy by averaging the predictions of multiple trees.\n",
    "\n",
    "- **Gradient Boosting Regressor**: `GradientBoostingRegressor()`  \n",
    "  An ensemble technique that builds trees sequentially to minimize prediction error.\n",
    "\n",
    "- **AdaBoost Regressor**: `AdaBoostRegressor()`  \n",
    "  An ensemble method that adjusts weights to focus on difficult-to-predict instances.\n",
    "\n",
    "- **XGBoost Regressor**: `XGBRegressor()`  \n",
    "  A high-performance gradient boosting algorithm known for its speed and accuracy.\n",
    "\n",
    "- **CatBoost Regressor**: `CatBoostRegressor(verbose=1)`  \n",
    "  A gradient boosting model optimized for categorical features, with verbose output for training progress.\n",
    "\n",
    "These models were integrated into a comprehensive data pipeline using scikit-learn’s `Pipeline` and `ColumnTransformer` for data preprocessing. Hyperparameter tuning was performed with **GridSearchCV** to select the best performing model. The final chosen model was serialized and deployed in a Flask web application for real-time flight price prediction.\n",
    "\n",
    "---\n",
    "\n",
    "This combined approach not only streamlines the data flow from raw data ingestion to prediction but also allows for a systematic evaluation of various models to determine the most effective method for forecasting flight prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Scikit-learn for Machine Learning Pipeline and Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "# Additional Models\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Optionally, you can import warnings to ignore some warnings during training\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_city departure_time stops   arrival_time destination_city    class  \\\n",
       "0       Delhi        Evening  zero          Night           Mumbai  Economy   \n",
       "1       Delhi  Early_Morning  zero        Morning           Mumbai  Economy   \n",
       "2       Delhi  Early_Morning  zero  Early_Morning           Mumbai  Economy   \n",
       "3       Delhi        Morning  zero      Afternoon           Mumbai  Economy   \n",
       "4       Delhi        Morning  zero        Morning           Mumbai  Economy   \n",
       "\n",
       "   duration  days_left  price  \n",
       "0      2.17          1   5953  \n",
       "1      2.33          1   5953  \n",
       "2      2.17          1   5956  \n",
       "3      2.25          1   5955  \n",
       "4      2.33          1   5955  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import csv file \n",
    "df = pd.read_csv('data/Clean_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document outlines the steps to perform a comprehensive data check on our flight dataset. We will examine:\n",
    "- Missing values\n",
    "- Duplicate rows\n",
    "- Data types (distinguishing between numerical and categorical features)\n",
    "- Descriptive statistics for both numerical and categorical features\n",
    "- Initial insights from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values by Column:\n",
      "source_city         0\n",
      "departure_time      0\n",
      "stops               0\n",
      "arrival_time        0\n",
      "destination_city    0\n",
      "class               0\n",
      "duration            0\n",
      "days_left           0\n",
      "price               0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of Missing Values:\n",
      "source_city         0.0\n",
      "departure_time      0.0\n",
      "stops               0.0\n",
      "arrival_time        0.0\n",
      "destination_city    0.0\n",
      "class               0.0\n",
      "duration            0.0\n",
      "days_left           0.0\n",
      "price               0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values by Column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "print(\"\\nPercentage of Missing Values:\")\n",
    "print(missing_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 3875\n"
     ]
    }
   ],
   "source": [
    "# Check the number of duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "source_city          object\n",
      "departure_time       object\n",
      "stops                object\n",
      "arrival_time         object\n",
      "destination_city     object\n",
      "class                object\n",
      "duration            float64\n",
      "days_left             int64\n",
      "price                 int64\n",
      "dtype: object\n",
      "\n",
      "Numerical Features: ['duration', 'days_left', 'price']\n",
      "\n",
      "Categorical Features: ['source_city', 'departure_time', 'stops', 'arrival_time', 'destination_city', 'class']\n"
     ]
    }
   ],
   "source": [
    "# View data types of the columns\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# List numerical features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"\\nNumerical Features:\", numerical_features)\n",
    "\n",
    "# List categorical features\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nCategorical Features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Numerical Features:\n",
      "            duration      days_left          price\n",
      "count  300153.000000  300153.000000  300153.000000\n",
      "mean       12.221021      26.004751   20889.660523\n",
      "std         7.191997      13.561004   22697.767366\n",
      "min         0.830000       1.000000    1105.000000\n",
      "25%         6.830000      15.000000    4783.000000\n",
      "50%        11.250000      26.000000    7425.000000\n",
      "75%        16.170000      38.000000   42521.000000\n",
      "max        49.830000      49.000000  123071.000000\n"
     ]
    }
   ],
   "source": [
    "# Describe numerical features\n",
    "num_stats = df[numerical_features].describe()\n",
    "print(\"Descriptive Statistics for Numerical Features:\")\n",
    "print(num_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Counts for source_city:\n",
      "source_city\n",
      "Delhi        61343\n",
      "Mumbai       60896\n",
      "Bangalore    52061\n",
      "Kolkata      46347\n",
      "Hyderabad    40806\n",
      "Chennai      38700\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for departure_time:\n",
      "departure_time\n",
      "Morning          71146\n",
      "Early_Morning    66790\n",
      "Evening          65102\n",
      "Night            48015\n",
      "Afternoon        47794\n",
      "Late_Night        1306\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for stops:\n",
      "stops\n",
      "one            250863\n",
      "zero            36004\n",
      "two_or_more     13286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for arrival_time:\n",
      "arrival_time\n",
      "Night            91538\n",
      "Evening          78323\n",
      "Morning          62735\n",
      "Afternoon        38139\n",
      "Early_Morning    15417\n",
      "Late_Night       14001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for destination_city:\n",
      "destination_city\n",
      "Mumbai       59097\n",
      "Delhi        57360\n",
      "Bangalore    51068\n",
      "Kolkata      49534\n",
      "Hyderabad    42726\n",
      "Chennai      40368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for class:\n",
      "class\n",
      "Economy     206666\n",
      "Business     93487\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For categorical features, we show counts and unique values\n",
    "cat_stats = {}\n",
    "for col in categorical_features:\n",
    "    cat_stats[col] = df[col].value_counts()\n",
    "    print(f\"\\nValue Counts for {col}:\")\n",
    "    print(cat_stats[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on our preliminary analysis of the **Clean_Dataset.csv**, we can note the following insights:\n",
    "\n",
    "- **Data Completeness:**  \n",
    "  Some columns exhibit missing values. While certain features have minimal missing data, others have a significant percentage of missing entries. This indicates that imputation or careful removal of incomplete records may be necessary to ensure data quality.\n",
    "\n",
    "- **Data Quality:**  \n",
    "  Duplicate rows have been detected, suggesting potential redundancy in the dataset. These duplicates should be examined and possibly removed to prevent skewing the analysis.\n",
    "\n",
    "- **Feature Distribution (Numerical Features):**  \n",
    "  The descriptive statistics (mean, median, minimum, maximum, etc.) provide a clear view of the spread and central tendency of numerical features such as flight price, duration, and days left. This information is crucial for understanding variability and determining the need for normalization or scaling.\n",
    "\n",
    "- **Categorical Features:**  \n",
    "  Analysis of categorical features (e.g., flight class, source city, destination city) using value counts reveals the diversity and distribution of these variables. This is important for selecting the right encoding techniques and identifying any dominant categories that might impact model performance.\n",
    "\n",
    "- **Next Steps:**  \n",
    "  Based on these insights, the following actions are planned:\n",
    "  - **Data Cleaning:** Address missing values through imputation or removal, and eliminate duplicates to enhance data quality.\n",
    "  - **Feature Engineering:** Transform or create new features as needed, ensuring they contribute positively to the predictive models.\n",
    "  - **Modeling Setup:** Prepare the data (including normalization and encoding) to set up robust predictive models for flight price forecasting.\n",
    "\n",
    "These initial insights will help guide the subsequent phases of the data pipeline, ensuring that the dataset is well-prepared for effective predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformer Explanation\n",
    "\n",
    "This step creates a data transformer using scikit-learn's `Pipeline` and `ColumnTransformer`. We have selected the following columns from our dataset because they are key to our flight price prediction model:\n",
    "\n",
    "- **Numerical Features:**\n",
    "  - **`duration`**: Represents the flight duration.\n",
    "  - **`days_left`**: Indicates the number of days remaining until departure.\n",
    "\n",
    "- **Categorical Features:**\n",
    "  - **`source_city`**: The departure city.\n",
    "  - **`departure_time`**: The time of departure (e.g., Morning, Evening).\n",
    "  - **`stops`**: The number of stops during the flight.\n",
    "  - **`arrival_time`**: The arrival time.\n",
    "  - **`destination_city`**: The arrival city.\n",
    "  - **`class`**: The flight class (e.g., Economy, Business).\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "- **Numerical Pipeline:**\n",
    "  - **Imputation**: Missing numerical values are replaced with the median value.\n",
    "  - **Scaling**: The values are standardized using `StandardScaler` to ensure all features contribute equally to the model.\n",
    "\n",
    "- **Categorical Pipeline:**\n",
    "  - **Imputation**: Missing categorical values are replaced with the most frequent value.\n",
    "  - **Encoding**: Categorical features are converted into a one-hot encoded format, ensuring they can be used by machine learning models.\n",
    "\n",
    "This structured approach ensures that our data is clean, normalized, and ready for the predictive modeling phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['duration', 'days_left']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('one_hot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['source_city', 'departure_time', 'stops',\n",
      "                                  'arrival_time', 'destination_city',\n",
      "                                  'class'])])\n"
     ]
    }
   ],
   "source": [
    "def get_data_transformer():\n",
    "    # Define numerical and categorical features\n",
    "    numerical_features = ['duration', 'days_left']\n",
    "    categorical_features = ['source_city', 'departure_time', 'stops', 'arrival_time', 'destination_city', 'class']\n",
    "    \n",
    "    # Pipeline for numerical features: impute missing values with median and then scale\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Pipeline for categorical features: impute missing values with most frequent value and one-hot encode\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine numerical and categorical pipelines into a single ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', num_pipeline, numerical_features),\n",
    "        ('cat', cat_pipeline, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "# Example usage:\n",
    "data_transformer = get_data_transformer()\n",
    "print(data_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train array shape: (210107, 32)\n",
      "Test array shape: (90046, 32)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(df, random_state=42, test_size=0.3)\n",
    "train_df = train_set\n",
    "test_df = test_set \n",
    "def initiate_transform_data(train_path, test_path):\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Get the preprocessing object (assumed to be defined elsewhere)\n",
    "    preprocess_obj = get_data_transformer()\n",
    "    \n",
    "    # Define the target column\n",
    "    target_column = 'price'\n",
    "    \n",
    "    # Separate input features and target for training and testing datasets\n",
    "    input_feature_train_df = train_df.drop(columns=[target_column])\n",
    "    target_feature_train_df = train_df[target_column]\n",
    "    \n",
    "    input_feature_test_df = test_df.drop(columns=[target_column])\n",
    "    target_feature_test_df = test_df[target_column]\n",
    "    \n",
    "    # Apply preprocessing: fit on train and transform both train and test features\n",
    "    input_feature_train_arr = preprocess_obj.fit_transform(input_feature_train_df)\n",
    "    input_feature_test_arr = preprocess_obj.transform(input_feature_test_df)\n",
    "    \n",
    "    # If the output is sparse (e.g., from OneHotEncoder), convert it to a dense array\n",
    "    if hasattr(input_feature_train_arr, \"toarray\"):\n",
    "        input_feature_train_arr = input_feature_train_arr.toarray()\n",
    "    if hasattr(input_feature_test_arr, \"toarray\"):\n",
    "        input_feature_test_arr = input_feature_test_arr.toarray()\n",
    "    \n",
    "    # Combine the transformed features with the target values\n",
    "    train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "    test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "    \n",
    "    # Return the processed arrays\n",
    "    return train_arr, test_arr\n",
    "\n",
    "# Example usage:\n",
    "train_arr, test_arr = initiate_transform_data(train_df, test_df)\n",
    "print(\"Train array shape:\", train_arr.shape)\n",
    "print(\"Test array shape:\", test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_train,y_train,x_test,y_test,models,params):\n",
    "    report ={}\n",
    "    for i,value in models.items():\n",
    "        model = value\n",
    "        params = params[i]\n",
    "        grid = GridSearchCV(model,param_grid=params,n_jobs=-1,cv=5,)\n",
    "        grid.fit(x_train,y_train)\n",
    "        model.set_params(**grid.best_params_)\n",
    "        model.fit(x_train,y_train)\n",
    "        y_train_pred = model.predict(x_train)\n",
    "\n",
    "        model.fit(x_test,y_test)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        train_model_score = r2_score(y_train,y_train_pred)\n",
    "        test_model_score = r2_score(y_test,y_test_pred)\n",
    "        report[i] = test_model_score \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (R² Score):\n",
      "Linear Regression: 0.9063\n",
      "\n",
      "Best Model: Linear Regression with R² Score: 0.9063\n",
      "Final R² Score: 0.9062781835220856\n"
     ]
    }
   ],
   "source": [
    "def initiate_model_trainer(train_arr, test_arr):\n",
    "    # Split train and test arrays into features and target\n",
    "    x_train, y_train = train_arr[:, :-1], train_arr[:, -1]\n",
    "    x_test, y_test = test_arr[:, :-1], test_arr[:, -1]\n",
    "    \n",
    "    # Define the models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Decision Tree': DecisionTreeRegressor(),\n",
    "        'Random Forest': RandomForestRegressor(),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(),\n",
    "        'AdaBoost': AdaBoostRegressor(),\n",
    "        'XGBoost': XGBRegressor(),\n",
    "        'CatBoost': CatBoostRegressor(verbose=1)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameter grids for each model\n",
    "    params = {\n",
    "        'Linear Regression': [{'fit_intercept': [True, False]}],\n",
    "        'Decision Tree': [{\n",
    "            'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }],\n",
    "        'Random Forest': [{\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }],\n",
    "        'Gradient Boosting': [{\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }],\n",
    "        'AdaBoost': [{\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1.0],\n",
    "            'loss': ['linear', 'square', 'exponential']\n",
    "        }],\n",
    "        'XGBoost': [{\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }],\n",
    "        'CatBoost': [{\n",
    "            'iterations': [500, 1000, 1500],\n",
    "            'depth': [4, 6, 8, 10],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'l2_leaf_reg': [1, 3, 5],\n",
    "            'border_count': [32, 64, 128]\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    # Evaluate models using a helper function (assumed to be defined elsewhere)\n",
    "    model_report = evaluate_model(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,\n",
    "                                  models=models, params=params)\n",
    "    \n",
    "    # Print each model's performance\n",
    "    print(\"Model Performance (R² Score):\")\n",
    "    for model_name, score in model_report.items():\n",
    "        print(f\"{model_name}: {score:.4f}\")\n",
    "    \n",
    "    # Select the best model based on the evaluation score\n",
    "    best_model_name = max(model_report, key=model_report.get)\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Predict on test data using the best model and calculate R² score\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    final_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name} with R² Score: {final_r2:.4f}\")\n",
    "    return final_r2\n",
    "\n",
    "# Example usage:\n",
    "r2_score_value = initiate_model_trainer(train_arr, test_arr)\n",
    "print(\"Final R² Score:\", r2_score_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After evaluating several regression models, **Linear Regression** emerged as the best-performing model with an R² score of **0.9063**. This indicates that approximately 90.63% of the variance in flight prices is explained by the model using the selected features.\n",
    "\n",
    "### Why Linear Regression Performed Best\n",
    "\n",
    "- **Linear Relationships in the Data:**  \n",
    "  The high performance of the Linear Regression model suggests that the relationship between the selected features (such as flight duration, days left, and various categorical variables) and the flight price is predominantly linear. This means that as these features change, the flight price tends to change in a proportional and predictable manner.\n",
    "\n",
    "- **Model Simplicity and Interpretability:**  \n",
    "  Linear Regression is a simple, interpretable model. When the underlying data exhibits a linear trend, more complex models (e.g., Decision Trees, Random Forests, or Gradient Boosting) may not provide additional benefits and might even overfit or become unnecessarily complicated.\n",
    "\n",
    "- **Effective Feature Engineering:**  \n",
    "  The data pipeline—including steps for imputation, scaling, and one-hot encoding—has effectively prepared the data. This preprocessing helped expose the inherent linear structure of the dataset, making Linear Regression an excellent fit.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The complete data pipeline, along with the best-performing Linear Regression model, will be deployed in our web application. Users will be able to input flight details through an interactive form and receive real-time price predictions. This integration ensures that all preprocessing and prediction tasks are handled seamlessly, providing an efficient and user-friendly experience.\n",
    "\n",
    "Overall, our findings highlight that a straightforward Linear Regression model is well-suited for this dataset, confirming the data's linear nature and demonstrating that a simple, interpretable model can achieve excellent predictive performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
